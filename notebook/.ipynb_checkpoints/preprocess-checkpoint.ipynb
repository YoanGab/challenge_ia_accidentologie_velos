{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from enum import Enum\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint\n",
    "import shutil\n",
    "import time\n",
    "from shapely.geometry import shape\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gpd.read_file('/Users/Pierr/Downloads/france-20230101.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(df,detail=True):\n",
    "    total = 0\n",
    "    for col in df.columns:\n",
    "        miss = df[col].isnull().sum()\n",
    "        pct = df[col].isna().mean() * 100\n",
    "        total += miss\n",
    "        if miss != 0:\n",
    "            if pct>10: color=Fore.RED\n",
    "            else: color=Fore.YELLOW\n",
    "            print(color+'{} => {} [{}%]'.format(col, miss, round(pct, 2)))\n",
    "        \n",
    "        elif (total == 0) and(detail):\n",
    "            print(Fore.GREEN+'{} => no missing values [{}%]'.format(col, 0))\n",
    "        total=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mid_local => no missing values [0%]\n",
      "\u001b[32mid_osm => no missing values [0%]\n",
      "\u001b[31mnum_iti => 240797 [88.12%]\n",
      "\u001b[33mcode_com_d => 202 [0.07%]\n",
      "\u001b[33mame_d => 7 [0.0%]\n",
      "\u001b[31mregime_d => 38129 [13.95%]\n",
      "\u001b[33msens_d => 59 [0.02%]\n",
      "\u001b[31mlargeur_d => 261200 [95.59%]\n",
      "\u001b[31mlocal_d => 189049 [69.18%]\n",
      "\u001b[32mstatut_d => no missing values [0%]\n",
      "\u001b[31mrevet_d => 113407 [41.5%]\n",
      "\u001b[33mcode_com_g => 204 [0.07%]\n",
      "\u001b[32mame_g => no missing values [0%]\n",
      "\u001b[31mregime_g => 85280 [31.21%]\n",
      "\u001b[31msens_g => 54376 [19.9%]\n",
      "\u001b[31mlargeur_g => 262304 [95.99%]\n",
      "\u001b[31mlocal_g => 189764 [69.44%]\n",
      "\u001b[32mstatut_g => no missing values [0%]\n",
      "\u001b[31mrevet_g => 113406 [41.5%]\n",
      "\u001b[31maccess_ame => 247076 [90.42%]\n",
      "\u001b[32mdate_maj => no missing values [0%]\n",
      "\u001b[31mtrafic_vit => 39726 [14.54%]\n",
      "\u001b[31mlumiere => 210753 [77.13%]\n",
      "\u001b[31md_service => 271462 [99.34%]\n",
      "\u001b[32msource => no missing values [0%]\n",
      "\u001b[32mproject_c => no missing values [0%]\n",
      "\u001b[32mref_geo => no missing values [0%]\n",
      "\u001b[32mgeometry => no missing values [0%]\n"
     ]
    }
   ],
   "source": [
    "missing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(line):\n",
    "    # On crée un objet géométrique à partir de la linestring\n",
    "    line = shape(line)\n",
    "    # On récupère les coordonnées du premier point de la linestring\n",
    "    longitude_dep, latitude_dep = line.coords[0]\n",
    "    longitude_fin, latitude_fin = line.coords[1]\n",
    "    return latitude_dep, longitude_dep, latitude_fin, longitude_fin\n",
    "\n",
    "def create_lat_long(df):\n",
    "    df[\"coords\"] = df[\"geometry\"].apply(get_coords)\n",
    "    df[[\"latitude_dep\", \"longitude_dep\", \"latitude_fin\", \"longitude_fin\"]] = pd.DataFrame(df[\"coords\"].tolist(), index=df.index)\n",
    "    df = df.drop(\"coords\", axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocessing(df):\n",
    "    #hesite to drop : regime_d, code_com_d, regime_g, code_come_g\n",
    "    df = create_lat_long(df)\n",
    "    df = df.drop(columns=[\"id_local\", \"id_osm\", \"num_iti\", \"sens_d\", \n",
    "                                \"largeur_d\", \"local_d\", \"statut_d\", \"revet_d\", \"sens_g\", \"largeur_g\", \"local_g\", \"statut_g\", \"revet_g\",\"ref_geo\", 'project_c',\n",
    "                                \"d_service\", \"lumiere\", \"source\", \"geometry\", \"trafic_vit\", \"access_ame\"])\n",
    "    df = df.dropna(subset=[\"code_com_d\"])\n",
    "    return df\n",
    "\n",
    "def is_paris(code):\n",
    "    # Les codes postaux de Paris vont de 75001 à 75020\n",
    "    return code.startswith(\"75\") and \"0\" <= code[-2:] <= \"9\"\n",
    "\n",
    "def amenagement_paris(df):\n",
    "    df = preprocessing(df)\n",
    "    df = df[df[\"code_com_d\"].apply(is_paris)]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m geo_fin \u001b[38;5;241m=\u001b[39m amenagement_paris(\u001b[43mtest\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "geo_fin = amenagement_paris(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(items):\n",
    "    start_time = time.time()\n",
    "    for i, item in enumerate(items):\n",
    "        bar_length = int(50 * (i+1) / len(items))\n",
    "        bar = \"#\" * bar_length + \"-\" * (50 - bar_length)\n",
    "        print(f\"{i+1}/{len(items)} [{bar}]\", end='\\r')\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccidentData:\n",
    "    def __init__(self):\n",
    "        # Répertoire où se trouvent les données\n",
    "        self.data_dir = \"..\\data\"\n",
    "        # Nom du fichier JSON contenant les URLs de téléchargement des données\n",
    "        self.accident_corporels_urls_filename = \"accident_corporels_urls.json\"\n",
    "        # Liste des catégories de données disponibles\n",
    "        self.categories = [\"usagers\", \"vehicules\", \"lieux\", \"caracteristiques\"]\n",
    "        # Lecture des URLs à partir du fichier JSON\n",
    "        self.read_urls()\n",
    "        # Années pour lesquelles il y a des données disponibles\n",
    "        self.years = list(self.urls[\"usagers\"].keys())\n",
    "        # Vérification et contrôle des données\n",
    "        self.check_and_control_data()\n",
    "        self.df_final = None\n",
    "    \n",
    "    def read_urls(self):\n",
    "        \"\"\"\n",
    "        Charge les URLs des données dans l'attribut self.urls.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.data_dir, self.accident_corporels_urls_filename),\"r\") as file:\n",
    "            self.urls = json.load(file)\n",
    "    \n",
    "    def download_data(self):\n",
    "        \"\"\"\n",
    "        Télécharge les données manquantes.\n",
    "        \"\"\"\n",
    "        # S'il y a des fichiers manquants\n",
    "        if len(self.filenames) > 0:\n",
    "            print(\"[Check] Checking completed, some data is missing!\")\n",
    "            print(\"[Download] Downloading missing data...\")\n",
    "            # Pour chaque catégorie de données\n",
    "            for filename in self.filenames.keys():\n",
    "                filename_dir = os.path.join(self.data_dir,filename) # Répertoire où stocker les données\n",
    "                print(f\"\\n[Download] Downloading {filename} files...\")\n",
    "                # Pour chaque année\n",
    "                for i, year in enumerate(self.filenames[filename]):\n",
    "                    # Téléchargement des données\n",
    "                    data = requests.get(self.urls[filename][year]).text\n",
    "\n",
    "                     # Création du répertoire s'il n'existe pas\n",
    "                    if not os.path.exists(filename_dir):\n",
    "                        os.makedirs(filename_dir)\n",
    "\n",
    "                    # Écriture des données dans un fichier CSV\n",
    "                    with open(os.path.join(filename_dir,f\"{year}.csv\"), 'w', encoding='utf-8') as f:\n",
    "                        f.write(data)\n",
    "\n",
    "                    # Affichage de la barre de progression\n",
    "                    bar_length = int(50 * (i+1) / len(self.filenames[filename]))\n",
    "                    bar = \"#\" * bar_length + \"-\" * (50 - bar_length)\n",
    "                    print(f\"{i+1}/{len(self.filenames[filename])} [{bar}]\", end='\\r')\n",
    "            print(\"\\n[Download] Download completed!\")\n",
    "        # S'il n'y a pas de fichiers manquants\n",
    "        else:\n",
    "            print(\"[Check] Checking completed, no data is missing!\")\n",
    "\n",
    "    def check_missing_data(self):\n",
    "        \"\"\"\n",
    "        Vérifie les données manquantes.\n",
    "        \"\"\"\n",
    "        print(\"[Check] Checking if data is in your computer...\")\n",
    "        # Dictionnaire des fichiers manquants par catégorie\n",
    "        self.filenames = {}\n",
    "        # Pour chaque catégorie de données\n",
    "        for categorie in self.categories:\n",
    "            # Chemin du répertoire de cette catégorie\n",
    "            filename_path = os.path.join(self.data_dir,categorie)\n",
    "            # Si le répertoire de cette catégorie n'existe pas, tous les fichiers de cette catégorie sont manquants\n",
    "            if not os.path.exists(filename_path):\n",
    "                self.filenames[categorie] = self.years\n",
    "            # Si le répertoire existe, vérification des fichiers manquants\n",
    "            else:\n",
    "                for year in self.years:\n",
    "                    # Si le fichier de cette année n'existe pas, il est considéré comme manquant\n",
    "                    if not os.path.exists(os.path.join(filename_path, f\"{year}.csv\")):\n",
    "                        # Ajout de l'année au dictionnaire des fichiers manquants\n",
    "                        if categorie in self.filenames:\n",
    "                            self.filenames[categorie].append(year)\n",
    "                        else:\n",
    "                            self.filenames[categorie] = [year]\n",
    "                            \n",
    "    def check_and_control_data(self):\n",
    "        \"\"\"\n",
    "        Vérifie les données manquantes et les télécharge si nécessaire.\n",
    "        \"\"\"\n",
    "        self.check_missing_data()\n",
    "        self.download_data()\n",
    "    \n",
    "    def reset_db(self):\n",
    "        \"\"\"\n",
    "        Réinitialise les données en supprimant tous les répertoires de données et en vérifiant les données manquantes.\n",
    "        \"\"\"\n",
    "        print(\"[Reset] Reseting data...\")\n",
    "        # Suppression de tous les répertoires de données\n",
    "        for categorie in self.categories:\n",
    "            categorie_path = os.path.join(self.data_dir, categorie)\n",
    "            if os.path.exists(categorie_path):\n",
    "                shutil.rmtree(categorie_path, ignore_errors=True) \n",
    "        print(\"[Reset] Data have been deleted\")\n",
    "        # Téléchargement des données\n",
    "        self.check_and_control_data()\n",
    "        print(\"[Reset] Data have been reset\")\n",
    "\n",
    "    def get_pd_file_from_year(self, cat, begin, end=None, merge=True):\n",
    "        \"\"\"\n",
    "        Récupère un DataFrame pandas à partir de l'année demandée.\n",
    "        \n",
    "        Parameters:\n",
    "        cat (str) : Catégorie de données à récupérer.\n",
    "        begin (int) : Année de début.\n",
    "        end (int) : Année de fin (optionnel, par défaut None).\n",
    "        merge (bool) : Si True, fusionne les DataFrames de chaque année en un seul DataFrame. Si False, renvoie une liste de DataFrames.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame pandas ou liste de DataFrames\n",
    "        \"\"\"\n",
    "        # Si la catégorie de données demandée est valide\n",
    "        if cat.lower() in self.categories:\n",
    "            cat_path = os.path.join(self.data_dir, cat)\n",
    "            # Si une seule année est demandée\n",
    "            if end == None:\n",
    "                # Si l'année demandée existe dans les données\n",
    "                if str(begin) in self.years:\n",
    "                    # Chargement du fichier CSV en tant que DataFrame pandas\n",
    "                    return pd.read_csv(os.path.join(cat_path, f\"{begin}.csv\"), sep=None, engine='python')\n",
    "            # Si une plage d'années est demandée\n",
    "            else:\n",
    "                if str(begin) in self.years and str(end) in self.years:\n",
    "                    list_df = []\n",
    "                    for annee in range(begin,end+1):\n",
    "                        list_df.append(pd.read_csv(os.path.join(cat_path, f\"{str(annee)}.csv\"), sep=None, engine='python'))\n",
    "                    if merge:\n",
    "                        return pd.concat(list_df)\n",
    "                    else:\n",
    "                        return list_df\n",
    "        else:\n",
    "            raise ValueError(\"La catégorie de données demandée n'est pas valide\")\n",
    "            \n",
    "    def get_merge_df(self, begin, end, save=True, name=\"df_merge.csv\"):\n",
    "        name = os.path.join(self.data_dir ,name)\n",
    "        print(\"[Check] Checking if the file already exists...\")\n",
    "        if os.path.exists(name):\n",
    "            print(\"[Check] File already exists! loading file\")\n",
    "            df_final = pd.read_csv(name, index_col=0)\n",
    "            return df_final\n",
    "        print(\"[Check] File not found, merging...\")\n",
    "        # Pour chaque catégorie\n",
    "        for cat in progress_bar(self.categories):\n",
    "            # Récupération du DataFrame de la catégorie\n",
    "            df = self.get_pd_file_from_year(cat=cat, begin=begin, end=end)\n",
    "            # Si c'est la première catégorie, le DataFrame final est le DataFrame de la catégorie\n",
    "            if cat == self.categories[0]:\n",
    "                df[\"grav\"] = df.grav.map({1:0,2:3,3:2,4:1})\n",
    "                df = pd.DataFrame(df.groupby(\"Num_Acc\")[\"grav\"].max()).reset_index()\n",
    "                df_final = df\n",
    "            else:\n",
    "                # Fusion du DataFrame avec le DataFrame final\n",
    "                self.df_final = pd.merge(df_final, df, on=\"Num_Acc\")\n",
    "            # Enregistrement du DataFrame final dans un fichier CSV\n",
    "            if save:\n",
    "                self.df_final.to_csv(name)\n",
    "            return self.df_final\n",
    "    \n",
    "    def preprocess_df(self, save=True, name=\"df_velo_preprocess.csv\"):\n",
    "        path = os.path.join(self.data_dir ,name)\n",
    "        if self.df_final is not None:\n",
    "            Num_Acc =  self.df_final[self.df_final[\"catv\"] == 1][\"Num_Acc\"]\n",
    "            df_velo_acc = df_final[df_final[\"Num_Acc\"].isin(list(Num_Acc))]\n",
    "            df_velo_acc.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            selected_features = [\"Num_Acc\",\"jour\",\"mois\",\"an\",\"lum\",\"agg\",\"int\",\"atm\",\"col\",\n",
    "                     \"catr\",\"circ\",\"nbv\",\"vosp\",\"prof\",\"plan\",\"larrout\",\n",
    "                     \"surf\",\"infra\",\"situ\",\"vma\",\"catv\",\"obs\",\"obsm\",\n",
    "                     \"choc\",\"manv\",\"grav\",\"manv2\",\"catv2\",\"obs2\",\"obsm2\",\"choc2\"]\n",
    "\n",
    "            df_velo_acc_features = df_velo_acc_veh[selected_features].reset_index(drop=True)\n",
    "            \n",
    "            new_cols = [\"manv2\", \"catv2\", \"obs2\", \"obsm2\", \"choc2\"]\n",
    "            cols = list(df_velo_acc.columns)+new_cols\n",
    "            df_velo_acc_veh = pd.DataFrame(columns=cols)\n",
    "            index = -1\n",
    "            num_acc_inserted: list = []\n",
    "            df_velo_acc = df_velo_acc.sort_values(\"Num_Acc\")\n",
    "            for i, row in tqdm(df_velo_acc.iterrows()):\n",
    "                if row.Num_Acc in num_acc_inserted: # df_velo_acc_veh.Num_Acc.to_numpy():\n",
    "                    manv2 = row[\"manv\"]\n",
    "                    catv2 = row[\"catv\"]\n",
    "                    obs2 = row[\"obs\"]\n",
    "                    obsm2 = row[\"obsm\"]\n",
    "                    choc2 = row[\"choc\"]\n",
    "\n",
    "                    df_velo_acc_veh.loc[index, new_cols] = [manv2, catv2, obs2, obsm2, choc2]\n",
    "                else:\n",
    "                    index += 1\n",
    "                    df_velo_acc_veh = df_velo_acc_veh.append({col:val for col,val in zip(cols,list(row.to_numpy())+[None]*5)} , ignore_index=True)\n",
    "                    num_acc_inserted.append(row.Num_Acc)\n",
    "            \n",
    "            if save:\n",
    "                df_velo_acc_features.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] Checking if data is in your computer...\n",
      "[Check] Checking completed, no data is missing!\n"
     ]
    }
   ],
   "source": [
    "acc_data = AccidentData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea7e1a4d4824d679697f1117a5821ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1d80ad67319cdf01eb41b135f33e23f51f5b7f0fa6cd51dd5f6d9ed8aa4c9a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
