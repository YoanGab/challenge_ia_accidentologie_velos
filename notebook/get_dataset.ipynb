{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from enum import Enum\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint\n",
    "import shutil\n",
    "import time\n",
    "from shapely.geometry import shape\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccidentData:\n",
    "    def __init__(self):\n",
    "        # Répertoire où se trouvent les données\n",
    "        self.data_dir = \"..\\data\"\n",
    "        # Liste des catégories de données disponibles\n",
    "        self.categories = [\"usagers\", \"vehicules\", \"lieux\", \"caracteristiques\"]\n",
    "        self.selected_features = [\"Num_Acc\",\"jour\",\"mois\",\"an\",\"lum\",\"agg\",\"int\",\"atm\",\"col\",\n",
    "                                    \"catr\",\"circ\",\"nbv\",\"vosp\",\"prof\",\"plan\",\"larrout\",\n",
    "                                    \"surf\",\"infra\",\"situ\",\"vma\",\"catv\",\"obs\",\"obsm\",\n",
    "                                    \"choc\",\"manv\",\"grav\",\"manv2\",\"catv2\",\"obs2\",\"obsm2\",\"choc2\"]\n",
    "        # Lecture des URLs à partir du fichier JSON\n",
    "        self.read_urls()\n",
    "        # Années pour lesquelles il y a des données disponibles\n",
    "        self.years = list(self.urls[\"usagers\"].keys())\n",
    "        # Vérification et contrôle des données\n",
    "        self.check_and_control_data()\n",
    "        self.df_final = None\n",
    "    \n",
    "    def get_json_files(self):\n",
    "        \"\"\"\n",
    "        Charge les URLs des données dans l'attribut self.urls.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.data_dir, \"accident_corporels_urls.json\"),\"r\") as file:\n",
    "            self.urls = json.load(file)\n",
    "        with open(os.path.join(self.data_dir, \"description_features.json\"),\"r\") as file:\n",
    "            self.feat_desc = json.load(file)\n",
    "    \n",
    "    def download_data(self):\n",
    "        \"\"\"\n",
    "        Télécharge les données manquantes.\n",
    "        \"\"\"\n",
    "        # S'il y a des fichiers manquants\n",
    "        if len(self.filenames) > 0:\n",
    "            print(\"[Check] Checking completed, some data is missing!\")\n",
    "            print(\"[Download] Downloading missing data...\")\n",
    "            # Pour chaque catégorie de données\n",
    "            for filename in self.filenames.keys():\n",
    "                filename_dir = os.path.join(self.data_dir,filename) # Répertoire où stocker les données\n",
    "                print(f\"\\n[Download] Downloading {filename} files...\")\n",
    "                # Pour chaque année\n",
    "                for i, year in enumerate(self.filenames[filename]):\n",
    "                    # Téléchargement des données\n",
    "                    data = requests.get(self.urls[filename][year]).text\n",
    "\n",
    "                     # Création du répertoire s'il n'existe pas\n",
    "                    if not os.path.exists(filename_dir):\n",
    "                        os.makedirs(filename_dir)\n",
    "\n",
    "                    # Écriture des données dans un fichier CSV\n",
    "                    with open(os.path.join(filename_dir,f\"{year}.csv\"), 'w', encoding='utf-8') as f:\n",
    "                        f.write(data)\n",
    "\n",
    "                    # Affichage de la barre de progression\n",
    "                    bar_length = int(50 * (i+1) / len(self.filenames[filename]))\n",
    "                    bar = \"#\" * bar_length + \"-\" * (50 - bar_length)\n",
    "                    print(f\"{i+1}/{len(self.filenames[filename])} [{bar}]\", end='\\r')\n",
    "            print(\"\\n[Download] Download completed!\")\n",
    "        # S'il n'y a pas de fichiers manquants\n",
    "        else:\n",
    "            print(\"[Check] Checking completed, no data is missing!\")\n",
    "\n",
    "    def check_missing_data(self):\n",
    "        \"\"\"\n",
    "        Vérifie les données manquantes.\n",
    "        \"\"\"\n",
    "        print(\"[Check] Checking if data is in your computer...\")\n",
    "        # Dictionnaire des fichiers manquants par catégorie\n",
    "        self.filenames = {}\n",
    "        # Pour chaque catégorie de données\n",
    "        for categorie in self.categories:\n",
    "            # Chemin du répertoire de cette catégorie\n",
    "            filename_path = os.path.join(self.data_dir,categorie)\n",
    "            # Si le répertoire de cette catégorie n'existe pas, tous les fichiers de cette catégorie sont manquants\n",
    "            if not os.path.exists(filename_path):\n",
    "                self.filenames[categorie] = self.years\n",
    "            # Si le répertoire existe, vérification des fichiers manquants\n",
    "            else:\n",
    "                for year in self.years:\n",
    "                    # Si le fichier de cette année n'existe pas, il est considéré comme manquant\n",
    "                    if not os.path.exists(os.path.join(filename_path, f\"{year}.csv\")):\n",
    "                        # Ajout de l'année au dictionnaire des fichiers manquants\n",
    "                        if categorie in self.filenames:\n",
    "                            self.filenames[categorie].append(year)\n",
    "                        else:\n",
    "                            self.filenames[categorie] = [year]\n",
    "                            \n",
    "    def check_and_control_data(self):\n",
    "        \"\"\"\n",
    "        Vérifie les données manquantes et les télécharge si nécessaire.\n",
    "        \"\"\"\n",
    "        self.check_missing_data()\n",
    "        self.download_data()\n",
    "    \n",
    "    def reset_db(self):\n",
    "        \"\"\"\n",
    "        Réinitialise les données en supprimant tous les répertoires de données et en vérifiant les données manquantes.\n",
    "        \"\"\"\n",
    "        print(\"[Reset] Reseting data...\")\n",
    "        # Suppression de tous les répertoires de données\n",
    "        for categorie in self.categories:\n",
    "            categorie_path = os.path.join(self.data_dir, categorie)\n",
    "            if os.path.exists(categorie_path):\n",
    "                shutil.rmtree(categorie_path, ignore_errors=True) \n",
    "        print(\"[Reset] Data have been deleted\")\n",
    "        # Téléchargement des données\n",
    "        self.check_and_control_data()\n",
    "        print(\"[Reset] Data have been reset\")\n",
    "\n",
    "    def get_pd_file_from_year(self, cat, begin, end=None, merge=True):\n",
    "        \"\"\"\n",
    "        Récupère un DataFrame pandas à partir de l'année demandée.\n",
    "        \n",
    "        Parameters:\n",
    "        cat (str) : Catégorie de données à récupérer.\n",
    "        begin (int) : Année de début.\n",
    "        end (int) : Année de fin (optionnel, par défaut None).\n",
    "        merge (bool) : Si True, fusionne les DataFrames de chaque année en un seul DataFrame. Si False, renvoie une liste de DataFrames.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame pandas ou liste de DataFrames\n",
    "        \"\"\"\n",
    "        # Si la catégorie de données demandée est valide\n",
    "        if cat.lower() in self.categories:\n",
    "            cat_path = os.path.join(self.data_dir, cat)\n",
    "            # Si une seule année est demandée\n",
    "            if end == None:\n",
    "                # Si l'année demandée existe dans les données\n",
    "                if str(begin) in self.years:\n",
    "                    # Chargement du fichier CSV en tant que DataFrame pandas\n",
    "                    return pd.read_csv(os.path.join(cat_path, f\"{begin}.csv\"), sep=None, engine='python')\n",
    "            # Si une plage d'années est demandée\n",
    "            else:\n",
    "                if str(begin) in self.years and str(end) in self.years:\n",
    "                    list_df = []\n",
    "                    for annee in range(begin,end+1):\n",
    "                        list_df.append(pd.read_csv(os.path.join(cat_path, f\"{str(annee)}.csv\"), sep=None, engine='python'))\n",
    "                    if merge:\n",
    "                        return pd.concat(list_df)\n",
    "                    else:\n",
    "                        return list_df\n",
    "        else:\n",
    "            raise ValueError(\"La catégorie de données demandée n'est pas valide\")\n",
    "            \n",
    "    def get_merge_df(self, begin, end, save=True, name=\"df_merge.csv\"):\n",
    "        name = os.path.join(self.data_dir ,name)\n",
    "        print(\"[Check] Checking if the file already exists...\")\n",
    "        if os.path.exists(name):\n",
    "            print(\"[Check] File already exists! loading file\")\n",
    "            df_final = pd.read_csv(name, index_col=0)\n",
    "            return df_final\n",
    "        print(\"[Check] File not found, merging...\")\n",
    "        # Pour chaque catégorie\n",
    "        for cat in tqdm(self.categories):\n",
    "            # Récupération du DataFrame de la catégorie\n",
    "            df = self.get_pd_file_from_year(cat=cat, begin=begin, end=end)\n",
    "            # Si c'est la première catégorie, le DataFrame final est le DataFrame de la catégorie\n",
    "            if cat == self.categories[0]:\n",
    "                df[\"grav\"] = df.grav.map({1:0,2:3,3:2,4:1})\n",
    "                df = pd.DataFrame(df.groupby(\"Num_Acc\")[\"grav\"].max()).reset_index()\n",
    "                df_final = df\n",
    "            else:\n",
    "                # Fusion du DataFrame avec le DataFrame final\n",
    "                self.df_final = pd.merge(df_final, df, on=\"Num_Acc\")\n",
    "            # Enregistrement du DataFrame final dans un fichier CSV\n",
    "            if save:\n",
    "                self.df_final.to_csv(name)\n",
    "            return self.df_final\n",
    "    \n",
    "    def preprocess_df(self, begin=2005, end=2021, save=True, name=\"dataset_velo_acc_preprocess.csv\"):\n",
    "        path = os.path.join(self.data_dir ,name)\n",
    "        if os.path.exists(path):\n",
    "            print(\"[Check] File already exists! loading file\")\n",
    "            self.df_final = pd.read_csv(path, index_col=0)\n",
    "            return self.df_final\n",
    "        if self.df_final is not None:\n",
    "            print(\"[Preprocessing] File not found, merging...\")\n",
    "            self.get_merge_df(2005,2021)\n",
    "            print(\"[Preprocessing] File not found, preprocessing...\")\n",
    "            # Sélection des vélos\n",
    "            Num_Acc =  self.df_final[self.df_final[\"catv\"] == 1][\"Num_Acc\"]\n",
    "            df_velo_acc = self.df_final[self.df_final[\"Num_Acc\"].isin(list(Num_Acc))]\n",
    "            df_velo_acc.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            \n",
    "            # Ajout du second véhicule\n",
    "            new_cols = [\"manv2\", \"catv2\", \"obs2\", \"obsm2\", \"choc2\"]\n",
    "            cols = list(df_velo_acc.columns)+new_cols\n",
    "            df_velo_acc_veh = pd.DataFrame(columns=cols)\n",
    "            index = -1\n",
    "            num_acc_inserted: list = []\n",
    "            df_velo_acc = df_velo_acc.sort_values(\"Num_Acc\")\n",
    "            for i, row in tqdm(df_velo_acc.iterrows()):\n",
    "                if row.Num_Acc in num_acc_inserted:\n",
    "                    manv2 = row[\"manv\"]\n",
    "                    catv2 = row[\"catv\"]\n",
    "                    obs2 = row[\"obs\"]\n",
    "                    obsm2 = row[\"obsm\"]\n",
    "                    choc2 = row[\"choc\"]\n",
    "                    df_velo_acc_veh.loc[index, new_cols] = [manv2, catv2, obs2, obsm2, choc2]\n",
    "                else:\n",
    "                    index += 1\n",
    "                    df_velo_acc_veh = df_velo_acc_veh.append({col:val for col,val in zip(cols,list(row.to_numpy())+[None]*5)} , ignore_index=True)\n",
    "                    num_acc_inserted.append(row.Num_Acc)\n",
    "\n",
    "            # Sélection des features\n",
    "            self.df_final = df_velo_acc_veh[self.selected_features].reset_index(drop=True)\n",
    "            \n",
    "            if save:\n",
    "                self.df_final.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] Checking if data is in your computer...\n",
      "[Check] Checking completed, no data is missing!\n",
      "[Check] File already exists! loading file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flavi\\AppData\\Local\\Temp\\ipykernel_17040\\148556076.py:173: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df_final = pd.read_csv(path, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "acc_data = AccidentData()\n",
    "df_final = acc_data.preprocess_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vosp       float64\n",
       "prof       float64\n",
       "plan       float64\n",
       "larrout     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = [\"vosp\",\"prof\",\"plan\",\"larrout\"]\n",
    "df_selected = df_final[selected_features]\n",
    "df_selected.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3574476506165663\n",
      "0.2510791455887991\n",
      "0.27334232105972717\n",
      "11.281245748351907\n"
     ]
    }
   ],
   "source": [
    "for col in selected_features:\n",
    "    print(100*df_selected[col].isna().sum()/df_selected.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0    64744\n",
       " 1.0     8733\n",
       " 2.0     3576\n",
       " 3.0     3346\n",
       "-1.0      163\n",
       "Name: vosp, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.vosp.value_counts()\n",
    "df_selected[\"vosp\"].replace({-1:np.NaN}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    64388\n",
       " 0.0     5687\n",
       " 2.0     5588\n",
       " 3.0     4236\n",
       " 4.0      727\n",
       "-1.0        4\n",
       "Name: plan, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.plan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    62770\n",
       " 2.0    11674\n",
       " 0.0     4202\n",
       " 4.0     1117\n",
       " 3.0      881\n",
       "-1.0        4\n",
       "Name: prof, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.prof.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        50.0\n",
       "1        50.0\n",
       "2         0.0\n",
       "3        52.0\n",
       "4        50.0\n",
       "         ... \n",
       "80846    -1.0\n",
       "80847    -1.0\n",
       "80848    -1.0\n",
       "80849    -1.0\n",
       "80850    -1.0\n",
       "Name: larrout, Length: 80851, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.larrout.apply(lambda x: str(x).replace(\",\",\".\")).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        50.0\n",
       "1        50.0\n",
       "2         0.0\n",
       "3        52.0\n",
       "4        50.0\n",
       "         ... \n",
       "80846    -1.0\n",
       "80847    -1.0\n",
       "80848    -1.0\n",
       "80849    -1.0\n",
       "80850    -1.0\n",
       "Name: larrout, Length: 80851, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "961ba29095b76c930f4fc5dd59aa7edba0948bb17ac8528d55d140f484815b9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
